# Training configuration

# Optimization
batch_size: 64
total_steps: 10000
learning_rate: 1.0e-4
weight_decay: 0.0

# Scheduler (optional cosine annealing)
use_scheduler: true
warmup_steps: 100

# Logging
log_interval: 100     # Log every N steps
eval_interval: 500    # Evaluate on val set every N steps

# Checkpointing
save_best: true       # Save best model by val loss

# Symmetry penalty
lambda_sym: 0.0              # Weight for symmetry penalty (0 = disabled)
sym_layers: []               # List of layers to penalize (1-indexed, -1 for output)
sym_penalty_type: "raw"      # "raw" (RawOrbitVariancePenalty) or "pca" (PCAOrbitVariancePenalty)
n_augmentations_train: 4     # Number of rotation pairs per sample for penalty
