# 8-layer MLP with width 128
# Architecture: [2, 128, 128, 128, 128, 128, 128, 128, 128, 1]

input_dim: 2          # (x, y) coordinates
hidden_dim: 128       # Width of hidden layers
num_layers: 8         # Number of hidden layers
output_dim: 1         # Predicted radius (regression)
activation: "relu"    # ReLU activation (no batchnorm/layernorm/dropout)
